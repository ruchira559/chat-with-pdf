{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHLxwR4x51lU2qhOz2gLr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruchira559/chat-with-pdf/blob/main/research_and_prototyping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Core Framework & 2026 Migration Support\n",
        "!pip install -q -U langchain langchain-community langchain-groq langchain-huggingface langchain-classic\n",
        "\n",
        "# 2. Specialized Utilities\n",
        "!pip install -q -U langchain-text-splitters chromadb pypdf sentence-transformers\n",
        "\n",
        "print(\"All libraries installed. PLEASE RESTART RUNTIME (Runtime > Restart session).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEq7Gjq2boHF",
        "outputId": "6b1c5a25-8a39-49ed-b16e-60fbd7a69e38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Block 1: All libraries installed. PLEASE RESTART RUNTIME (Runtime > Restart session).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Updated 2026 Imports\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Legacy Support Import for RetrievalQA\n",
        "from langchain_classic.chains import RetrievalQA  # FIXED for v1.0\n",
        "\n",
        "# API Keys\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "print(\"Modules imported using 2026-standard paths.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGgYHBcAb_f3",
        "outputId": "99456548-6f3d-4ae4-ec10-009be01e65ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Block 2: Modules imported using 2026-standard paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Chunk the PDF\n",
        "loader = PyPDFLoader(\"data.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "TCEYcc_0cIJ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification Print\n",
        "print(f\"Document loaded: {len(pages)} pages found.\")\n",
        "print(f\"Document split into {len(chunks)} smaller chunks.\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Sample from Chunk #1:\\n{chunks[0].page_content[:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lx91u0xc9XV",
        "outputId": "8ed690f7-a3f2-4328-c69d-9e01012061c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document loaded: 12 pages found.\n",
            "Document split into 57 smaller chunks.\n",
            "------------------------------\n",
            "Sample from Chunk #1:\n",
            "Vol.:(0123456789)\n",
            "The International Journal of Life Cycle Assessment \n",
            "https://doi.org/10.1007/s11367-024-02405-8\n",
            "DATA AVAILABILITY , DATA QUALITY\n",
            "Testing the use of a large language model (LLM) for pe...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Convert text to Vectors\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embed_model,\n",
        "    collection_name=\"pdf_knowledge_base\"\n",
        ")\n",
        "print(\"Vector Database created successfully!\")"
      ],
      "metadata": {
        "id": "wznE_nqZcS9C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup the Brain (Groq Llama 3.1)\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=groq_api_key,\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "JK7fIQ89cWoN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Final Chain Assembly\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 7}),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(f\"Vector Store ready with {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3F6khE6ccJn",
        "outputId": "ec03764f-03f2-4c0c-d7cd-4ec2a916724a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Block 3: Logic Complete. Vector Store ready with 57 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_fact(query):\n",
        "    print(f\"\\n Querying PDF: {query}\")\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    print(f\" Answer: {response['result']}\")\n",
        "\n",
        "# Test cases based on the MacMaster & Sinistore (2024) paper\n",
        "verification_tests = [\n",
        "    \"What was the success rate for technology coverage according to the results?\",\n",
        "    \"Does the study conclude that LLMs can reduce practitioner bias?\",\n",
        "    \"What were the success rates for temporal and geographic coverage?\"\n",
        "]\n",
        "\n",
        "for test in verification_tests:\n",
        "    check_fact(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsltSVpAcg4u",
        "outputId": "ffbd6d58-fe64-4575-d622-499ad2b46a1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Querying PDF: What was the success rate for technology coverage according to the results?\n",
            " Answer: According to the results, the initial technology coverage test had a success rate of 73%. However, when the test was repeated for two scenarios where contextual clues in the prompt were obscured, the model had 100% success in reasoning and scoring.\n",
            "\n",
            " Querying PDF: Does the study conclude that LLMs can reduce practitioner bias?\n",
            " Answer: Yes, the study concludes that LLMs can reduce practitioner bias. According to the text, outsourcing DQA to artificial intelligence (A.I.) can \"eliminate practitioner's biases\" and reduce liability for practitioners.\n",
            "\n",
            " Querying PDF: What were the success rates for temporal and geographic coverage?\n",
            " Answer: According to the text, the success rates for temporal and geographic coverage are as follows:\n",
            "\n",
            "For temporal coverage, the LLM was successful on the initial attempt, as seen in Table 5. This suggests that the temporal coverage test had a 100% success rate.\n",
            "\n",
            "For geographic coverage, the test underwent four iterations of prompt refinement, and the results in Table 6 are the results of the last test. The text does not provide a specific success rate for geographic coverage, but it mentions that the prompt refinement process was successful in improving the results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0l9tDyOdipi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}